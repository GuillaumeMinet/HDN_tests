{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# We import all our dependencies.\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from models.lvae import LadderVAE\n",
    "from lib.gaussianMixtureNoiseModel import GaussianMixtureNoiseModel\n",
    "from boilerplate import boilerplate\n",
    "import lib.utils as utils\n",
    "import training\n",
    "from tifffile import imread\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised: False\n",
      "Noise level: 0\n",
      "Clean: avg\n",
      "Noise_model_name: GMM_Noise0_SigAVG_Clip-3.npz\n",
      "Saving name: Vim_fixed_mltplSNR_30nm_Noise0_SigAVG_Clip-3\n",
      "\n",
      "Found 25 files.\n",
      "\n",
      "Signal img_and_denoised_00.tiff:\tObservation rec_c01_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_01.tiff:\tObservation rec_c02_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_02.tiff:\tObservation rec_c03_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_03.tiff:\tObservation rec_c04_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_04.tiff:\tObservation rec_c05_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_05.tiff:\tObservation rec_c07_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_06.tiff:\tObservation rec_c08_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_07.tiff:\tObservation rec_c09_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_08.tiff:\tObservation rec_c10_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_09.tiff:\tObservation rec_c11_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_10.tiff:\tObservation rec_c12_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_11.tiff:\tObservation rec_c13_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_12.tiff:\tObservation rec_c14_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_13.tiff:\tObservation rec_c15_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_14.tiff:\tObservation rec_c16_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_15.tiff:\tObservation rec_c17_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_16.tiff:\tObservation rec_c19_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_17.tiff:\tObservation rec_c21_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_18.tiff:\tObservation rec_c22_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_19.tiff:\tObservation rec_c23_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_20.tiff:\tObservation rec_c24_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_21.tiff:\tObservation rec_c25_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_22.tiff:\tObservation rec_c26_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_23.tiff:\tObservation rec_c27_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "Signal img_and_denoised_24.tiff:\tObservation rec_c29_rec_CAM.tiff:\t Shape: (1416, 1392)\n",
      "\n",
      "\n",
      "Concatenated arrays:\tSignal: (25, 1416, 1392)\tObservation: (25, 1416, 1392)\n"
     ]
    }
   ],
   "source": [
    "# Data paths\n",
    "data_path_signal = Path(r\"E:\\dl_monalisa\\Data\\Vim_fixed_mltplSNR_30nm\\inference\\N2V\\Vim_fixed_Avg1-3_no_clipping\")\n",
    "data_path_obs = Path(r\"E:\\dl_monalisa\\Data\\Vim_fixed_mltplSNR_30nm\\dump\\rec\\timelapses_gathered\")\n",
    "noiseModelsDir = Path(r\"E:\\dl_monalisa\\Data\\Vim_fixed_mltplSNR_30nm\\noise_models\\final\")\n",
    "\n",
    "# Choose parameters\n",
    "supervised = False\n",
    "noise_level = 0 # \"all\",list of int, or int\n",
    "clean = \"avg\" # \"n2v\" or \"avg\"\n",
    "display = True # display images or not\n",
    "clip = -3 # False or clip value\n",
    "\n",
    "# Saving model name\n",
    "data_name = str(data_path_signal).split(\"\\\\\")[3]\n",
    "modelName = f\"{data_name}_Noise{noise_level}_Sig{clean.upper()}_Clip{clip}\"\n",
    "if supervised:\n",
    "    modelName=f\"{modelName}_supervised\"\n",
    "\n",
    "# Get noise model name (enter n_gauss and n_coeff manually)\n",
    "model_name = f\"GMM_Noise{noise_level}_Sig{clean.upper()}_Clip{clip}.npz\"\n",
    "assert os.path.exists(noiseModelsDir/model_name)\n",
    "\n",
    "print(f\"Supervised: {supervised}\")\n",
    "print(f\"Noise level: {noise_level}\")\n",
    "print(f\"Clean: {clean}\")\n",
    "print(f\"Noise_model_name: {model_name}\")\n",
    "print(f\"Saving name: {modelName}\")\n",
    "\n",
    "signal = []\n",
    "observation = []\n",
    "filters = ['tif','tiff']\n",
    "\n",
    "files_signal = os.listdir(data_path_signal)\n",
    "files_obs = os.listdir(data_path_obs)\n",
    "\n",
    "for f in files_signal:\n",
    "    if f.split('.')[-1] not in filters:\n",
    "        print(f\"removing {f} in signals because not in filters\")\n",
    "        files_signal.remove(f)\n",
    "\n",
    "for f in files_obs:\n",
    "    if f.split('.')[-1] not in filters:\n",
    "        print(f\"Removing {f} in observations because not in filters\")\n",
    "        files_obs.remove(f)\n",
    "\n",
    "assert len(files_obs) == len(files_signal)\n",
    "print(f\"\\nFound {len(files_signal)} files.\\n\")\n",
    "\n",
    "for i in range (len(files_obs)):\n",
    "    file_signal = files_signal [i]\n",
    "    file_obs = files_obs [i]\n",
    "    \n",
    "    # n2v or avg signal selection\n",
    "    if clean == \"n2v\":\n",
    "        im_signal = imread(data_path_signal / file_signal)[1]\n",
    "    elif clean == \"avg\":\n",
    "        im_signal = imread(data_path_signal / file_signal)[0]\n",
    "    \n",
    "    # noise level selection\n",
    "    if noise_level == \"all\":\n",
    "        im_obs  = imread(data_path_obs / file_obs)[:5]\n",
    "    elif isinstance(noise_level,int) or isinstance(noise_level,list):\n",
    "        im_obs = imread(data_path_obs / file_obs)[noise_level]\n",
    "\n",
    "    if not isinstance(clip,bool):\n",
    "        im_obs[im_obs<clip] = 0\n",
    "        im_signal[im_signal<clip] = 0\n",
    "\n",
    "    observation.append(im_obs)\n",
    "    signal.append(im_signal)\n",
    "\n",
    "    print(f\"Signal {file_signal}:\\tObservation {file_obs}:\\t Shape: {im_obs.shape}\")\n",
    "\n",
    "signal = np.stack(signal)\n",
    "observation = np.stack(observation)\n",
    "\n",
    "if len(observation.shape) == 4:\n",
    "    nrepeat = observation.shape[1]\n",
    "    observation = np.reshape(observation,(observation.shape[0]*observation.shape[1],observation.shape[2],observation.shape[3]))    \n",
    "    signal = np.repeat(signal,nrepeat,axis=0)\n",
    "\n",
    "\n",
    "signal = (signal - np.mean(signal))/np.std(signal)\n",
    "observation = (observation - np.mean(observation))/np.std(observation)\n",
    "\n",
    "\n",
    "print(f\"\\n\\nConcatenated arrays:\\tSignal: {signal.shape}\\tObservation: {observation.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = observation[:int(0.85*observation.shape[0])]\n",
    "val_data= observation[int(0.85*observation.shape[0]):]\n",
    "print(\"Shape of training images:\", train_data.shape, \"Shape of validation images:\", val_data.shape)\n",
    "train_data = utils.augment_data(train_data) ### Data augmentation disabled for fast training, but can be enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gt = signal[:int(0.85*signal.shape[0])]\n",
    "val_data_gt= signal[int(0.85*signal.shape[0]):]\n",
    "print(\"Shape of training images:\", train_data.shape, \"Shape of validation images:\", val_data.shape)\n",
    "train_data_gt = utils.augment_data(train_data_gt) ### Data augmentation disabled for fast training, but can be enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patch_size = 64\n",
    "\n",
    "img_width = observation.shape[2]\n",
    "img_height = observation.shape[1]\n",
    "num_patches = int(float(img_width*img_height)/float(patch_size**2)*1)\n",
    "\n",
    "### choose correct lines depending on if gt or not ###\n",
    "\n",
    "# train_images = utils.extract_patches(train_data, patch_size, num_patches)\n",
    "# val_images = utils.extract_patches(val_data, patch_size, num_patches)\n",
    "\n",
    "train_images,train_images_gt = utils.extract_patches_supervised(train_data,train_data_gt, patch_size, num_patches)\n",
    "val_images,val_images_gt  = utils.extract_patches_supervised(val_data,val_data_gt, patch_size, num_patches)\n",
    "\n",
    "\n",
    "val_images = val_images[:1000] # We limit validation patches to 1000 to speed up training but it is not necessary\n",
    "val_images_gt = val_images_gt [:1000] \n",
    "test_images = val_images[:100]\n",
    "test_images_gt = val_images_gt[:100]\n",
    "img_shape = (train_images.shape[1], train_images.shape[2])\n",
    "print(\"Shape of training images:\", train_images.shape, \"Shape of validation images:\", val_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "idx = random.randrange(train_images.shape[0])\n",
    "\n",
    "plt.figure(figsize=(5,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(train_images[idx])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(train_images_gt[idx])\n",
    "\n",
    "idx = random.randrange(val_images.shape[0])\n",
    "plt.figure(figsize=(5,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(val_images[idx])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(val_images_gt[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Hierarchical DivNoising model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>model_name</code> specifies the name of the model with which the weights will be saved and wil be loaded later for prediction.<br>\n",
    "<code>directory_path</code> specifies the directory where the model weights and the intermediate denoising and generation results will be saved. <br>\n",
    "<code>gaussian_noise_std</code> is only applicable if dataset is synthetically corrupted with Gaussian noise of known std. For real datasets, it should be set to ```None```.<br>\n",
    "<code>noiseModel</code> specifies a noise model for training. If noisy data is generated synthetically using Gaussian noise, set it to None. Else set it to the GMM based noise model (.npz file)  generated from '1-CreateNoiseModel.ipynb'.<br>\n",
    "<code>batch_size</code> specifies the batch size used for training. The default batch size of $64$ works well for most microscopy datasets.<br>\n",
    "<code>virtual_batch</code> specifies the virtual batch size used for training. It divides the <code>batch_size</code> into smaller mini-batches of size <code>virtual_batch</code>. Decrease this if batches do not fit in memory.<br>\n",
    "<code>test_batch_size</code> specifies the batch size used for testing every $1000$ training steps. Decrease this if test batches do not fit in memory, it does not have any consequence on training. It is just for intermediate visual debugging.<br>\n",
    "<code>lr</code> specifies the learning rate.<br>\n",
    "<code>max_epochs</code> specifies the total number of training epochs. Around $150-200$ epochs work well generally.<br>\n",
    "<code>steps_per_epoch</code> specifies how many steps to take per epoch of training. Around $400-500$ steps work well for most datasets.<br>\n",
    "<code>num_latents</code> specifies the number of stochastic layers. The default setting of $6$ works well for most datasets but quite good results can also be obtained with as less as $4$ layers. However, more stochastic layers may improve performance for some datasets at the cost of increased training time.<br>\n",
    "<code>z_dims</code> specifies the number of bottleneck dimensions (latent space dimensions) at each stochastic layer per pixel. The default setting of $32$ works well for most datasets.<br>\n",
    "<code>blocks_per_layer</code> specifies how many residual blocks to use per stochastic layer. Usually, setting it to be $4$ or more works well. However, more residual blocks improve performance at the cost of increased training time.<br>\n",
    "<code>batchnorm</code> specifies if batch normalization is used or not. Turning it to True is recommended.<br>\n",
    "<code>free_bits</code> specifies the threshold below which KL loss is not optimized for. This prevents the [KL-collapse problem](https://arxiv.org/pdf/1511.06349.pdf%3Futm_campaign%3DRevue%2520newsletter%26utm_medium%3DNewsletter%26utm_source%3Drevue). The default setting of $1.0$ works well for most datasets.<br>\n",
    "\n",
    "**__Note:__** With these settings, training will take approximately $24$ hours on Tesla P100/Titan Xp GPU needing about 6 GB GPU memory. We optimized the code to run on less GPU memory. For faster training, consider increasing ```virtual_batch_size``` but since we have not tested with different settings of ```virtual_batch_size```, we do not yet know how this affects results. To reduce traing time, also consider reducing either ```num_latents``` or ```blocks_per_layer``` to $4$. These settings will bring down the training time to around $12-15$ hours while still giving good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Vim_RepeatSupervisedSingleToAvg_NormHist_withAugment\"\n",
    "directory_path = \"./Trained_model/\" \n",
    "\n",
    "# Data-specific\n",
    "gaussian_noise_std = None\n",
    "noise_model_params= np.load(r\"E:\\dl_monalisa\\n2v_results\\Vimentin\\noise_models\\GMM_Single_AvgGT_normalized.npz\")\n",
    "noiseModel = GaussianMixtureNoiseModel(params = noise_model_params, device = device)\n",
    "\n",
    "# Training-specific\n",
    "batch_size=64\n",
    "virtual_batch = 8\n",
    "lr=3e-4\n",
    "max_epochs = 500\n",
    "steps_per_epoch=400\n",
    "test_batch_size=100\n",
    "\n",
    "# Model-specific\n",
    "num_latents = 4\n",
    "z_dims = [32]*int(num_latents)\n",
    "blocks_per_layer = 4\n",
    "batchnorm = True\n",
    "free_bits = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose make_data_manager for supervised training or make_data_manager 2 for supervised\n",
    "\n",
    "# train_loader, val_loader, test_loader, data_mean, data_std = boilerplate._make_datamanager(train_images,val_images,\n",
    "#                                                                                            test_images,batch_size,\n",
    "#                                                                                            test_batch_size)\n",
    "\n",
    "train_loader, val_loader, test_loader, data_mean, data_std = boilerplate._make_datamanager_supervised(train_images,train_images_gt,val_images,\n",
    "                                                                                           val_images_gt, test_images,test_images_gt,\n",
    "                                                                                           batch_size,test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx, (x, y) = next(enumerate(train_loader))\n",
    "\n",
    "x2 = x.cpu().numpy()\n",
    "x2 = x2[0]\n",
    "\n",
    "y2 = y.cpu().numpy()\n",
    "y2 = y2[0]\n",
    "\n",
    "plt.figure(figsize=(10,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(x2)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network\n",
    "\n",
    "model = LadderVAE(z_dims=z_dims,blocks_per_layer=blocks_per_layer,data_mean=data_mean,data_std=data_std,noiseModel=noiseModel,\n",
    "                  device=device,batchnorm=batchnorm,free_bits=free_bits,img_shape=img_shape).cuda()\n",
    "\n",
    "model.train() # Model set in training mode\n",
    "\n",
    "training.train_network(model=model,lr=lr,max_epochs=max_epochs,steps_per_epoch=steps_per_epoch,directory_path=directory_path,\n",
    "                       train_loader=train_loader,val_loader=val_loader,test_loader=test_loader,\n",
    "                       virtual_batch=virtual_batch,gaussian_noise_std=gaussian_noise_std,\n",
    "                       model_name=model_name,val_loss_patience=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory_path = \"./Trained_model/\" \n",
    "trainHist=np.load(directory_path+\"model/train_loss.npy\")\n",
    "reconHist=np.load(directory_path+\"model/train_reco_loss.npy\")\n",
    "klHist=np.load(directory_path+\"model/train_kl_loss.npy\")\n",
    "valHist=np.load(directory_path+\"model/val_loss.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 3))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(trainHist,label='training')\n",
    "plt.plot(valHist,label='validation')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(reconHist,label='training')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"reconstruction loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(klHist,label='training')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"KL loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
