{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# We import all our dependencies.\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from models.lvae import LadderVAE\n",
    "from lib.gaussianMixtureNoiseModel import GaussianMixtureNoiseModel\n",
    "from boilerplate import boilerplate\n",
    "import lib.utils as utils\n",
    "from lib import histNoiseModel\n",
    "from lib.utils import plotProbabilityDistribution\n",
    "import training\n",
    "from tifffile import imread\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from pystackreg import StackReg\n",
    "from skimage.transform import warp,AffineTransform\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center(img,crop_size):\n",
    "\n",
    "    if type(crop_size) == tuple:\n",
    "        crop_x,crop_y = crop_size\n",
    "    elif type(crop_size) == int:\n",
    "        crop_x = crop_size\n",
    "        crop_y = crop_size\n",
    "    \n",
    "    y,x = img.shape[-2::]\n",
    "    startx = x//2-(crop_x//2)\n",
    "    starty = y//2-(crop_y//2)        \n",
    "\n",
    "    return img[...,starty:starty+crop_y,startx:startx+crop_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded noise model: E:\\dl_monalisa\\Data\\Mito_fixed\\noise_models\\final\\GMM_Noiseall_SigAVG_Clip-5_noNorm.npz\n",
      "Trained model will be saved at: ./Trained_model/\n",
      "Model save name: Mito_fixed_GMMmito_clip-5_5Lat_6Blocks_betaKL0.05\n"
     ]
    }
   ],
   "source": [
    "#### ALL PARAMETERS DEFINED HERE ####\n",
    "\n",
    "supervised = False\n",
    "\n",
    "# Data paths\n",
    "data_path = Path(r\"E:\\dl_monalisa\\Data\\Mito_fixed\\dump\\train\")\n",
    "crop_size = (1200,1200)\n",
    "\n",
    "\n",
    "# Data parameters\n",
    "noise_level = \"all\" # \"all\",list of int, or int\n",
    "clean = \"avg\"\n",
    "clip = -5 # False or clip value\n",
    "registration = False\n",
    "augment = False\n",
    "normGMM = False # put True if data was normalized to create the GMM\n",
    "display = True # display images or not\n",
    "clip = -5 # False or clip value\n",
    "\n",
    "\n",
    "# Load GMM\n",
    "noiseModelPath = r\"E:\\dl_monalisa\\Data\\Mito_fixed\\noise_models\\final\\GMM_Noiseall_SigAVG_Clip-5_noNorm.npz\"\n",
    "noise_model_params = np.load(noiseModelPath)\n",
    "noiseModel = GaussianMixtureNoiseModel(params = noise_model_params, device = device)\n",
    "print(f\"Loaded noise model: {noiseModelPath}\")\n",
    "\n",
    "# Training prm\n",
    "patch_size = 64\n",
    "gaussian_noise_std = None\n",
    "\n",
    "# Training-specific\n",
    "beta = 0.05 # loss = recon_loss + beta * kl_loss\n",
    "batch_size=64\n",
    "virtual_batch = 8\n",
    "lr=1e-5\n",
    "max_epochs = 500\n",
    "steps_per_epoch=400\n",
    "test_batch_size=10\n",
    "\n",
    "# Model-specific\n",
    "num_latents = 5\n",
    "z_dims = [32]*int(num_latents)\n",
    "blocks_per_layer = 6\n",
    "batchnorm = True\n",
    "free_bits = 1.0\n",
    "\n",
    "# Model name for saving\n",
    "modelName = f\"Mito_fixed_GMMmito_clip{clip}_{num_latents}Lat_{blocks_per_layer}Blocks_betaKL{beta}\"\n",
    "if supervised:\n",
    "    modelName = modelName +\"_supervised\"\n",
    "\n",
    "save_model_basedir = \"./Trained_model/\" \n",
    "print(f\"Trained model will be saved at: {save_model_basedir}\")\n",
    "print(f\"Model save name: {modelName}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 22 files.\n",
      "\n",
      "File 12h08m06s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (20, 1200, 1200)\n",
      "File 12h11m06s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (17, 1200, 1200)\n",
      "File 12h14m16s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (19, 1200, 1200)\n",
      "File 12h17m11s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (19, 1200, 1200)\n",
      "File 12h19m41s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (20, 1200, 1200)\n",
      "File 12h23m09s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (20, 1200, 1200)\n",
      "File 12h25m12s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (20, 1200, 1200)\n",
      "File 12h28m54s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (20, 1200, 1200)\n",
      "File 12h31m33s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (19, 1200, 1200)\n",
      "File 12h34m06s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (18, 1200, 1200)\n",
      "File 12h39m29s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (20, 1200, 1200)\n",
      "File 12h44m23s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (19, 1200, 1200)\n",
      "File 12h47m09s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (19, 1200, 1200)\n",
      "File 14h14m42s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (16, 1200, 1200)\n",
      "File 14h21m45s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (16, 1200, 1200)\n",
      "File 14h25m08s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (20, 1200, 1200)\n",
      "File 14h27m52s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (17, 1200, 1200)\n",
      "File 14h31m19s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (18, 1200, 1200)\n",
      "File 14h34m59s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (20, 1200, 1200)\n",
      "File 14h38m21s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (19, 1200, 1200)\n",
      "File 14h43m32s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (18, 1200, 1200)\n",
      "File 14h48m20s_rec_scan00_CAM.hdf5_multi.0.reconstruction.tiff: shape: (19, 1200, 1200)\n"
     ]
    }
   ],
   "source": [
    "##### Load data ####\n",
    "signal = []\n",
    "observation = []\n",
    "filters = ['tif','tiff']\n",
    "\n",
    "files = os.listdir(data_path)\n",
    "\n",
    "for f in files:\n",
    "    if f.split('.')[-1] not in filters:\n",
    "        print(f\"removing {f} in signals because not in filters\")\n",
    "        files.remove(f)\n",
    "\n",
    "for f in files:\n",
    "    if f.split('.')[-1] not in filters:\n",
    "        print(f\"Removing {f} in observations because not in filters\")\n",
    "        files.remove(f)\n",
    "\n",
    "print(f\"\\nFound {len(files)} files.\\n\")\n",
    "if isinstance(noise_level,list) or noise_level == \"all\":\n",
    "    mltplNoise = True\n",
    "else:\n",
    "    mltplNoise = False\n",
    "\n",
    "\n",
    "for f in files:\n",
    "    \n",
    "    if clean == \"avg\":\n",
    "        im_signal = np.mean(imread(data_path / f)[0:5],axis=0)\n",
    "    else:\n",
    "        raise ValueError(\"avg gt is the only one available for now for actin\")\n",
    "    \n",
    "    # noise level selection\n",
    "    if noise_level == \"all\":\n",
    "        im_obs  = imread(data_path / f)\n",
    "    elif isinstance(noise_level,int) or isinstance(noise_level,list):\n",
    "        try:\n",
    "            im_obs = imread(data_path / f)[noise_level]\n",
    "        except IndexError:\n",
    "            print(f\"Image {f} ignored because of IndexError\") \n",
    "            continue\n",
    "\n",
    "    if not isinstance(clip,bool):\n",
    "        im_obs[im_obs<clip] = clip\n",
    "        im_signal[im_signal<clip] = clip\n",
    "\n",
    "    im_obs = crop_center(im_obs,crop_size)\n",
    "    im_signal = crop_center(im_signal,crop_size)\n",
    "    \n",
    "    observation.append(im_obs)\n",
    "    signal.append(im_signal)\n",
    "\n",
    "    print(f\"File {f}: shape: {im_obs.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Concatenated arrays:\tSignal: (413, 1200, 1200)\tObservation: (413, 1200, 1200)\n"
     ]
    }
   ],
   "source": [
    "full_signal = []\n",
    "full_obs = []\n",
    "for i,obs in enumerate(observation):\n",
    "    sig = signal[i]\n",
    "    sig = (sig - np.mean(sig))/np.std(sig)\n",
    "    for frame in obs:\n",
    "        norm_sig = sig * np.std(frame) + np.mean(frame)\n",
    "        full_signal.append(norm_sig)\n",
    "        full_obs.append(frame)\n",
    "\n",
    "observation = np.stack(full_obs)\n",
    "signal = np.stack(full_signal)\n",
    "minVal = np.min(signal)\n",
    "maxVal = np.max(signal)\n",
    "print(f\"\\n\\nConcatenated arrays:\\tSignal: {signal.shape}\\tObservation: {observation.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images: (351, 1200, 1200) Shape of validation images: (62, 1200, 1200)\n"
     ]
    }
   ],
   "source": [
    "# Split into train and val, define gt data if supervised\n",
    "\n",
    "train_data = observation[:int(0.85*observation.shape[0])]\n",
    "val_data= observation[int(0.85*observation.shape[0]):]\n",
    "print(\"Shape of training images:\", train_data.shape, \"Shape of validation images:\", val_data.shape)\n",
    "if augment:\n",
    "    train_data = utils.augment_data(train_data) ### Data augmentation disabled for fast training, but can be enabled\n",
    "### Optional part with GT data if supervised###\n",
    "if supervised:\n",
    "    train_data_gt = signal[:int(0.85*signal.shape[0])]\n",
    "    val_data_gt = signal[int(0.85*signal.shape[0]):]\n",
    "    print(\"Shape of GT training images:\", train_data.shape, \"Shape of validation images:\", val_data.shape)\n",
    "    if augment:\n",
    "        train_data_gt = utils.augment_data(train_data_gt) ### Data augmentation disabled for fast training, but can be enabled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351/351 [00:02<00:00, 123.99it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 132.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images: (123201, 64, 64) Shape of validation images: (1000, 64, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Patches extraction\n",
    "\n",
    "img_width = observation.shape[2]\n",
    "img_height = observation.shape[1]\n",
    "num_patches = int(float(img_width*img_height)/float(patch_size**2)*1)\n",
    "\n",
    "if not supervised:\n",
    "    train_images = utils.extract_patches(train_data, patch_size, num_patches)\n",
    "    val_images = utils.extract_patches(val_data, patch_size, num_patches)\n",
    "else:\n",
    "    train_images,train_images_gt = utils.extract_patches_supervised(train_data,train_data_gt, patch_size, num_patches)\n",
    "    val_images,val_images_gt  = utils.extract_patches_supervised(val_data,val_data_gt, patch_size, num_patches)\n",
    "\n",
    " # We limit validation patches to 1000 to speed up training but it is not necessary\n",
    "val_images = val_images[:1000]\n",
    "if supervised:\n",
    "    val_images_gt = val_images_gt [:1000] \n",
    "    \n",
    "img_shape = (train_images.shape[1], train_images.shape[2])\n",
    "print(\"Shape of training images:\", train_images.shape, \"Shape of validation images:\", val_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "# Choose make_data_manager for supervised training or make_data_manager 2 for supervised\n",
    "\n",
    "if supervised:\n",
    "    train_loader, val_loader, data_mean, data_std = boilerplate._make_datamanager_supervised(train_images, train_images_gt, \n",
    "                                                                                                        val_images, val_images_gt,\n",
    "                                                                                                        batch_size, upsamp=1)\n",
    "else:\n",
    "    train_loader,val_loader,data_mean,data_std = boilerplate._make_datamanager(train_images,val_images,batch_size)\n",
    "\n",
    "# Here we ensure that steps_per_epoch not bigger than len(train_loader)\n",
    "# It never goes into validation otherwise, and so it never saves the model.\n",
    "steps_per_epoch=min(len(train_loader)-1,steps_per_epoch)\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.010128969778677"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if supervised:\n",
    "    del train_images,train_images_gt,val_images,val_images_gt,train_data,train_data_gt,val_data,val_data_gt,observation,signal\n",
    "else:\n",
    "    del train_images,val_images,train_data,val_data,observation,signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if supervised: Display of paired dataset to check that it is still matching in the loader  \n",
    "\n",
    "if supervised:\n",
    "    batch_idx, (x, y) = next(enumerate(train_loader))\n",
    "    print(x.shape,y.shape)\n",
    "    x2 = x.cpu().numpy()\n",
    "    x2 = x2[0]\n",
    "    print(x2.shape)\n",
    "\n",
    "    y2 = y.cpu().numpy()\n",
    "    y2 = y2[0]\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(x2)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(y2)\n",
    "\n",
    "    batch_idx, (x, y) = next(enumerate(val_loader))\n",
    "\n",
    "    x2 = x.cpu().numpy()\n",
    "    x2 = x2[0]\n",
    "    print(x2.shape)\n",
    "\n",
    "    y2 = y.cpu().numpy()\n",
    "    y2 = y2[0]\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(x2)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network\n",
    "\n",
    "model = LadderVAE(z_dims=z_dims,blocks_per_layer=blocks_per_layer,data_mean=data_mean,data_std=data_std,noiseModel=noiseModel,\n",
    "                  device=device,batchnorm=batchnorm,free_bits=free_bits,img_shape=(64,64)).cuda()\n",
    "\n",
    "model.train() # Model set in training mode\n",
    "\n",
    "training.train_network(model=model,lr=lr,max_epochs=max_epochs,steps_per_epoch=steps_per_epoch,directory_path=save_model_basedir,\n",
    "                       train_loader=train_loader,val_loader=val_loader,test_loader=None,\n",
    "                       virtual_batch=virtual_batch,gaussian_noise_std=gaussian_noise_std,\n",
    "                       model_name=modelName,val_loss_patience=100,beta=beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory_path = \"./Trained_model/\" \n",
    "trainHist=np.load(directory_path+\"model/train_loss.npy\")\n",
    "reconHist=np.load(directory_path+\"model/train_reco_loss.npy\")\n",
    "klHist=np.load(directory_path+\"model/train_kl_loss.npy\")\n",
    "valHist=np.load(directory_path+\"model/val_loss.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 3))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(trainHist,label='training')\n",
    "plt.plot(valHist,label='validation')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(reconHist,label='training')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"reconstruction loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(klHist,label='training')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"KL loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_monalisa_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
